diff --git a/megatron/core/distributed/__init__.py b/megatron/core/distributed/__init__.py
index fe26e8b4..4451f277 100644
--- a/megatron/core/distributed/__init__.py
+++ b/megatron/core/distributed/__init__.py
@@ -11,3 +11,15 @@ from .finalize_model_grads import finalize_model_grads
 from .fsdp.mcore_fsdp_adapter import FullyShardedDataParallel
 from .torch_fully_sharded_data_parallel import TorchFullyShardedDataParallel
 from .torch_fully_sharded_data_parallel_config import TorchFullyShardedDataParallelConfig
+
+# Backward compatibility patch for FSDP module reorganization
+import sys
+import importlib.util
+
+spec = importlib.util.find_spec('megatron.core.distributed.fsdp.src.megatron_fsdp')
+if spec:
+    custom_fsdp = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(custom_fsdp)
+    sys.modules['megatron.core.distributed.custom_fsdp'] = custom_fsdp
+    if hasattr(custom_fsdp, 'MegatronFSDP'):
+        custom_fsdp.FullyShardedDataParallel = custom_fsdp.MegatronFSDP
diff --git a/megatron/core/extensions/transformer_engine.py b/megatron/core/extensions/transformer_engine.py
index 99c3edc0..26ea5cb4 100644
--- a/megatron/core/extensions/transformer_engine.py
+++ b/megatron/core/extensions/transformer_engine.py
@@ -404,6 +404,7 @@ class TELinear(te.pytorch.Linear):
         )
 
         for param in self.parameters():
+            setattr(param, "parallel_mode", parallel_mode)
             if is_expert:
                 # Reduce the gradient on the expert_data_parallel group for expert linear layers
                 setattr(param, "allreduce", not self.expert_parallel)
diff --git a/megatron/core/models/gpt/gpt_layer_specs.py b/megatron/core/models/gpt/gpt_layer_specs.py
index 002edb92..f7273488 100755
--- a/megatron/core/models/gpt/gpt_layer_specs.py
+++ b/megatron/core/models/gpt/gpt_layer_specs.py
@@ -80,6 +80,8 @@ def get_gpt_layer_with_transformer_engine_spec(
     use_te_op_fuser: Optional[bool] = False,
     use_kitchen: bool = False,
     use_te_activation_func: bool = False,
+    post_self_attn_layernorm: bool = False,
+    post_mlp_layernorm: bool = False,
 ) -> ModuleSpec:
     """Use this spec to use lower-level Transformer Engine modules (required for fp8 training).
 
@@ -182,9 +184,11 @@ def get_gpt_layer_with_transformer_engine_spec(
                     ),
                 ),
                 self_attn_bda=get_bias_dropout_add,
+                post_self_attn_layernorm=TENorm if post_self_attn_layernorm else IdentityOp,
                 pre_mlp_layernorm=backend.layer_norm() if num_experts else IdentityOp,
                 mlp=mlp,
                 mlp_bda=get_bias_dropout_add,
+                post_mlp_layernorm=TENorm if post_mlp_layernorm else IdentityOp,
                 sharded_state_dict_keys_map={
                     "mlp.0.weight": "mlp.linear_fc1.layer_norm_weight",
                     "mlp.0.bias": "mlp.linear_fc1.layer_norm_bias",
diff --git a/megatron/core/models/gpt/gpt_model.py b/megatron/core/models/gpt/gpt_model.py
index df9adc3e..a3ff966d 100644
--- a/megatron/core/models/gpt/gpt_model.py
+++ b/megatron/core/models/gpt/gpt_model.py
@@ -386,7 +386,7 @@ class GPTModel(LanguageModule):
             rotary_pos_sin=rotary_pos_sin,
             packed_seq_params=packed_seq_params,
             sequence_len_offset=sequence_len_offset,
-            **(extra_block_kwargs or {}),
+            # **(extra_block_kwargs or {}),
         )
 
         return self._postprocess(
@@ -443,7 +443,7 @@ class GPTModel(LanguageModule):
         if self.share_embeddings_and_output_weights:
             output_weight = self.shared_embedding_or_output_weight()
 
-        if mtp_in_postprocess:
+        if mtp_in_postprocess and labels is not None:
             hidden_states = self.mtp(
                 input_ids=input_ids,
                 position_ids=position_ids,
@@ -462,7 +462,7 @@ class GPTModel(LanguageModule):
                 output_weight=output_weight,
                 runtime_gather_output=runtime_gather_output,
                 compute_language_model_loss=self.compute_language_model_loss,
-                **(extra_block_kwargs or {}),
+                extra_block_kwargs=extra_block_kwargs,
             )
 
         if not self.post_process:
@@ -521,6 +521,12 @@ class GPTModel(LanguageModule):
             # [s b h] => [b s h]
             return logits.transpose(0, 1).contiguous()
 
+        # Optionally return logits even when labels are provided. This allows
+        # callers to trigger MTP loss computation while still obtaining logits
+        # for an external loss (e.g., RL policy loss).
+        if extra_block_kwargs and extra_block_kwargs.get('mtp_return_logits_when_labels', False):
+            return logits.transpose(0, 1).contiguous()
+
         loss = self.compute_language_model_loss(labels, logits)
 
         return loss
diff --git a/megatron/core/parallel_state.py b/megatron/core/parallel_state.py
index 57332ac3..f3abd642 100644
--- a/megatron/core/parallel_state.py
+++ b/megatron/core/parallel_state.py
@@ -9,6 +9,7 @@ from typing import Callable, List, Optional
 
 import numpy as np
 import torch
+import torch.distributed as dist
 
 from .utils import GlobalMemoryBuffer, is_torch_min_version
 
@@ -163,6 +164,213 @@ def get_nccl_options(pg_name, nccl_comm_cfgs):
         return None
 
 
+old_new_group = None
+
+
+def monkey_patch_torch_dist():
+    print("Applying monkey patch to torch.distributed", flush=True)
+    global old_new_group
+    if old_new_group is not None:
+        return
+
+    old_new_group = dist.new_group
+
+    def new_group(*args, **kwargs):
+        group = old_new_group(*args, **kwargs)
+        # skip none nccl group.
+        if (
+            len(args) >= 3 and args[2] == "gloo" or
+            "backend" in kwargs and kwargs["backend"] == "gloo"
+        ):
+            return group
+        
+        # Get ranks from arguments
+        if len(args) >= 1 and args[0] is not None:
+            ranks = args[0]
+        elif "ranks" in kwargs and kwargs["ranks"] is not None:
+            ranks = kwargs["ranks"]
+        else:
+            # If no ranks specified, use all ranks in world
+            ranks = list(range(dist.get_world_size()))
+
+        if len(ranks) == 1:
+            return group
+
+        group = ReloadableProcessGroup(group, ranks)
+        return group
+
+    dist.new_group = new_group
+
+    def get_new_function(func):
+        def new_function(*args, **kwargs):
+            args = (
+                arg.group if isinstance(arg, ReloadableProcessGroup) else arg
+                for arg in args
+            )
+            kwargs = {
+                k: (v.group if isinstance(v, ReloadableProcessGroup) else v)
+                for k, v in kwargs.items()
+            }
+            return func(*args, **kwargs)
+        return new_function
+
+    dist.get_rank = get_new_function(dist.get_rank)
+    dist.get_world_size = get_new_function(dist.get_world_size)
+    dist.get_backend = get_new_function(dist.get_backend)
+    dist.get_global_rank = get_new_function(dist.get_global_rank)
+    dist.get_group_rank = get_new_function(dist.get_group_rank)
+    dist.get_process_group_ranks = get_new_function(dist.get_process_group_ranks)
+
+    dist.all_reduce = get_new_function(dist.all_reduce)
+    dist.all_gather = get_new_function(dist.all_gather)
+    dist.all_gather_into_tensor = get_new_function(dist.all_gather_into_tensor)
+    dist.all_gather_object = get_new_function(dist.all_gather_object)
+    dist.all_to_all = get_new_function(dist.all_to_all)
+    dist.all_to_all_single = get_new_function(dist.all_to_all_single)
+    dist.broadcast = get_new_function(dist.broadcast)
+    dist.reduce = get_new_function(dist.reduce)
+    dist.reduce_scatter = get_new_function(dist.reduce_scatter)
+    dist.reduce_scatter_tensor = get_new_function(dist.reduce_scatter_tensor)
+    dist.scatter = get_new_function(dist.scatter)
+    dist.gather = get_new_function(dist.gather)
+    dist.barrier = get_new_function(dist.barrier)
+    dist.send = get_new_function(dist.send)
+    dist.recv = get_new_function(dist.recv)
+    dist._coalescing_manager = get_new_function(dist._coalescing_manager)
+
+    # p2p
+    old_isend = dist.isend
+    old_irecv = dist.irecv
+
+    dist.isend = get_new_function(dist.isend)
+    dist.irecv = get_new_function(dist.irecv)
+
+    def get_new_p2pop_function(func):
+        def new_function(*args, **kwargs):
+            def convert(arg):
+                if isinstance(arg, ReloadableProcessGroup):
+                    return arg.group
+                elif arg == dist.isend:
+                    arg = old_isend
+                elif arg == dist.irecv:
+                    arg = old_irecv
+                return arg
+
+            args = (convert(arg) for arg in args)
+            kwargs = {
+                k: convert(v)
+                for k, v in kwargs.items()
+            }
+            return func(*args, **kwargs)
+        return new_function
+    
+    dist.P2POp.__new__ = get_new_p2pop_function(dist.P2POp.__new__)
+    dist.P2POp.__init__ = get_new_p2pop_function(dist.P2POp.__init__)
+
+
+
+class ReloadableProcessGroup(torch.distributed.ProcessGroup):
+    GROUPS = []
+
+    def __init__(self, group, ranks):
+        super().__init__(
+            rank=dist.get_rank(group),
+            size=dist.get_world_size(group),
+        )
+        #print(f"Creating ReloadableProcessGroup with ranks: {ranks}", flush=True)
+        self.group = group
+        self.group_info = {
+            "ranks": ranks,
+        }
+        ReloadableProcessGroup.GROUPS.append(self)
+
+    def __getattr__(self, name):
+        return getattr(self.group, name)
+
+    @staticmethod
+    def destroy_process_groups():
+        for reloadable_group in ReloadableProcessGroup.GROUPS:
+            if reloadable_group.group is None:
+                continue
+            #print(f"Destroying process group: {reloadable_group.group_info['ranks']}")
+            dist.destroy_process_group(reloadable_group.group)
+            del reloadable_group.group
+            reloadable_group.group = None
+
+    @staticmethod
+    def reload_process_groups():
+        for reloadable_group in ReloadableProcessGroup.GROUPS:
+            if reloadable_group.group is not None:
+                continue
+            #print(f"Reloading process group: {reloadable_group.group_info['ranks']}")
+            group = old_new_group(
+                ranks=reloadable_group.group_info["ranks"],
+                backend="nccl"
+            )
+            reloadable_group.group = group
+
+    def rank(self) -> int: return self.group.rank()
+    def size(self) -> int: return self.group.size()
+    def name(self) -> str: return self.group.name()
+
+    def shutdown(self) -> None:
+        if self.group is not None:
+            self.group.shutdown()
+
+    def abort(self) -> None:
+        if self.group is not None:
+            self.group.abort()
+
+    def _fwd(self, method, *args, **kwargs):
+        inner = self.group
+        if inner is None:
+            raise RuntimeError("ReloadableProcessGroup: inner PG is None, call reload() first.")
+        return getattr(inner, method)(*args, **kwargs)
+
+    def barrier(self, *a, **kw): return self._fwd("barrier", *a, **kw)
+    def broadcast(self, *a, **kw): return self._fwd("broadcast", *a, **kw)
+    def allreduce(self, *a, **kw): return self._fwd("allreduce", *a, **kw)
+    def allreduce_coalesced(self, *a, **kw): return self._fwd("allreduce_coalesced", *a, **kw)
+    def reduce(self, *a, **kw): return self._fwd("reduce", *a, **kw)
+    def allgather(self, *a, **kw): return self._fwd("allgather", *a, **kw)
+    def _allgather_base(self, *a, **kw): return self._fwd("_allgather_base", *a, **kw)
+    def allgather_coalesced(self, *a, **kw): return self._fwd("allgather_coalesced", *a, **kw)
+    def allgather_into_tensor_coalesced(self, *a, **kw): return self._fwd("allgather_into_tensor_coalesced", *a, **kw)
+    def gather(self, *a, **kw): return self._fwd("gather", *a, **kw)
+    def scatter(self, *a, **kw): return self._fwd("scatter", *a, **kw)
+    def reduce_scatter(self, *a, **kw): return self._fwd("reduce_scatter", *a, **kw)
+    def _reduce_scatter_base(self, *a, **kw): return self._fwd("_reduce_scatter_base", *a, **kw)
+    def reduce_scatter_tensor_coalesced(self, *a, **kw): return self._fwd("reduce_scatter_tensor_coalesced", *a, **kw)
+    def alltoall_base(self, *a, **kw): return self._fwd("alltoall_base", *a, **kw)
+    def alltoall(self, *a, **kw): return self._fwd("alltoall", *a, **kw)
+    def send(self, *a, **kw): return self._fwd("send", *a, **kw)
+    def recv(self, *a, **kw): return self._fwd("recv", *a, **kw)
+    def recv_anysource(self, *a, **kw): return self._fwd("recv_anysource", *a, **kw)
+
+    def _start_coalescing(self, *a, **kw): return self._fwd("_start_coalescing", *a, **kw)
+    def _end_coalescing(self, *a, **kw): return self._fwd("_end_coalescing", *a, **kw)
+    def _get_backend_name(self): return self._fwd("_get_backend_name")
+    def _get_backend(self, *a, **kw): return self._fwd("_get_backend", *a, **kw)
+    def _set_default_backend(self, *a, **kw): return self._fwd("_set_default_backend", *a, **kw)
+    @property
+    def bound_device_id(self): return self.group.bound_device_id
+    @bound_device_id.setter
+    def bound_device_id(self, dev): self.group.bound_device_id = dev
+
+
+def destroy_process_groups():
+    """Destroy all reloadable process groups."""
+    ReloadableProcessGroup.destroy_process_groups()
+
+
+def reload_process_groups():
+    """Reload all reloadable process groups."""
+    ReloadableProcessGroup.reload_process_groups()
+
+
+monkey_patch_torch_dist()
+
+
 def create_group(
     ranks=None,
     timeout=None,
diff --git a/megatron/core/pipeline_parallel/p2p_communication.py b/megatron/core/pipeline_parallel/p2p_communication.py
index 63ee9d1f..b90b744c 100644
--- a/megatron/core/pipeline_parallel/p2p_communication.py
+++ b/megatron/core/pipeline_parallel/p2p_communication.py
@@ -26,22 +26,22 @@ def _batched_p2p_ops(
     ops = []
     if tensor_send_prev is not None:
         send_prev_op = torch.distributed.P2POp(
-            torch.distributed.isend, tensor_send_prev, prev_pipeline_rank, group
+            torch.distributed.isend, tensor_send_prev, prev_pipeline_rank,
         )
         ops.append(send_prev_op)
     if tensor_recv_prev is not None:
         recv_prev_op = torch.distributed.P2POp(
-            torch.distributed.irecv, tensor_recv_prev, prev_pipeline_rank, group
+            torch.distributed.irecv, tensor_recv_prev, prev_pipeline_rank,
         )
         ops.append(recv_prev_op)
     if tensor_send_next is not None:
         send_next_op = torch.distributed.P2POp(
-            torch.distributed.isend, tensor_send_next, next_pipeline_rank, group
+            torch.distributed.isend, tensor_send_next, next_pipeline_rank,
         )
         ops.append(send_next_op)
     if tensor_recv_next is not None:
         recv_next_op = torch.distributed.P2POp(
-            torch.distributed.irecv, tensor_recv_next, next_pipeline_rank, group
+            torch.distributed.irecv, tensor_recv_next, next_pipeline_rank,
         )
         ops.append(recv_next_op)
     if len(ops) > 0:
diff --git a/megatron/core/transformer/multi_token_prediction.py b/megatron/core/transformer/multi_token_prediction.py
index 06efbc06..3a0201d1 100755
--- a/megatron/core/transformer/multi_token_prediction.py
+++ b/megatron/core/transformer/multi_token_prediction.py
@@ -3,8 +3,10 @@
 from contextlib import nullcontext
 from dataclasses import dataclass
 from typing import List, Optional, Union
+import warnings
 
 import torch
+from functools import partial
 from torch import Tensor
 
 from megatron.core import InferenceParams, mpu, parallel_state, tensor_parallel
@@ -105,17 +107,19 @@ def tie_output_layer_state_dict(
     )
 
 
-def roll_tensor(tensor, shifts=-1, dims=-1, cp_group=None):
-    """Roll the tensor input along the sequence dimension with Context Parallelism (CP) support.
+def roll_tensor(tensor, shifts=-1, dims=-1, cp_group=None, packed_seq_params=None):
+    """Roll the tensor input along the sequence dimension with Context Parallelism (CP) and Packed Sequence support.
 
-    This function extends the original roll_tensor to support Context Parallelism, which allows
-    MTP to work with CP > 1. When CP is enabled, the sequence dimension is split across CP ranks,
-    and tensor rolling requires communication between adjacent CP ranks to properly handle the
-    boundary conditions.
+    This function extends the original roll_tensor to support Context Parallelism and Packed Sequences.
+    When CP is enabled, the sequence dimension is split across CP ranks, and tensor rolling requires 
+    communication between adjacent CP ranks to properly handle the boundary conditions.
+    When packed sequences are used, rolling is performed within each individual sequence boundary 
+    to prevent mixing tokens between different packed sequences.
 
     For CP=1 (default behavior): Uses standard torch.roll with zero padding
     For CP>1: Splits tensor into chunks, performs rolling within each chunk, then exchanges
     boundary elements between adjacent CP ranks to maintain sequence continuity.
+    For packed sequences: Rolls tensors within sequence boundaries defined by cu_seqlens.
 
     Args:
         tensor (Tensor): The input tensor to roll.
@@ -123,9 +127,16 @@ def roll_tensor(tensor, shifts=-1, dims=-1, cp_group=None):
         dims (int): The dimension to roll (typically -1 for sequence dimension).
         cp_group (ProcessGroup): The context parallelism process group. If None or size=1,
                                falls back to standard rolling behavior.
+        packed_seq_params (PackedSeqParams): Parameters for packed sequence processing.
+                                           If provided, rolling respects sequence boundaries.
     Returns:
         tuple: (rolled_tensor, sum_of_rolled_tensor)
     """
+    if packed_seq_params is not None:
+        if cp_group is not None and cp_group.size() > 1:
+            raise ValueError("CP > 1 and packed sequence are not yet supported together")
+        return _roll_tensor_packed_seq(tensor, shifts, dims, packed_seq_params, cp_group)
+    
     # Standard rolling behavior when CP is not enabled (cp_group is None or size=1)
     if cp_group is None or cp_group.size() == 1:
         rolled_tensor = torch.roll(tensor, shifts=shifts, dims=dims)
@@ -194,6 +205,70 @@ def roll_tensor(tensor, shifts=-1, dims=-1, cp_group=None):
     return rolled_tensor, rolled_tensor.sum()
 
 
+def _roll_tensor_packed_seq(tensor, shifts, dims, packed_seq_params, cp_group=None):
+    """Roll tensor with packed sequence support.
+    
+    This function handles rolling for packed sequences by respecting sequence boundaries
+    defined in packed_seq_params.cu_seqlens. Rolling is performed within each individual
+    sequence to prevent mixing tokens between different packed sequences.
+    
+    Args:
+        tensor (Tensor): The input tensor to roll.
+        shifts (int): The shift of the tensor (typically -1 for MTP).
+        dims (int): The dimension to roll (typically -1 for sequence dimension).
+        packed_seq_params (PackedSeqParams): Parameters for packed sequence processing.
+        cp_group (ProcessGroup): The context parallelism process group.
+        
+    Returns:
+        tuple: (rolled_tensor, sum_of_rolled_tensor)
+    """
+    # Use padded cu_seqlens if available for CP support, otherwise use regular cu_seqlens
+    assert cp_group is None or cp_group.size() == 1, "CP > 1 and packed sequence are not yet supported together"
+    assert dims == -1 or dims == tensor.dim() - 1, "This roll_tensor function with packed sequence only supports rolling the last dimension"
+    cu_seqlens = packed_seq_params.cu_seqlens_q
+    
+    # Clone the tensor to avoid modifying the original
+    rolled_tensor = tensor.clone()
+    
+    # For each sequence in the packed batch, roll within its boundaries
+    for i in range(len(cu_seqlens) - 1):
+        start_idx = cu_seqlens[i]
+        end_idx = cu_seqlens[i + 1]
+        seq_len = end_idx - start_idx
+        
+        if seq_len <= 0:
+            continue
+            
+        # Extract the sequence slice
+        seq_slice = tensor[..., start_idx:end_idx]
+        
+        # Roll within this sequence
+        rolled_seq = torch.roll(seq_slice, shifts=shifts, dims=dims)
+        
+        # Zero out the shifted elements at sequence boundaries
+        if shifts < 0:
+            # For negative shifts (left roll), zero out the rightmost elements
+            rolled_seq[..., shifts:] = 0
+        else:
+            # For positive shifts (right roll), zero out the leftmost elements
+            rolled_seq[..., :shifts] = 0
+        
+        # Put the rolled sequence back into the tensor
+        rolled_tensor[..., start_idx:end_idx] = rolled_seq
+    
+    # For packed sequences, calculate num_tokens properly by summing across all sequences
+    # This ensures we don't double-count tokens and properly handle sequence boundaries
+    num_tokens = torch.zeros_like(rolled_tensor.sum())
+    for i in range(len(cu_seqlens) - 1):
+        start_idx = cu_seqlens[i]
+        end_idx = cu_seqlens[i + 1]
+        if end_idx > start_idx:
+            seq_sum = rolled_tensor[..., start_idx:end_idx].sum()
+            num_tokens += seq_sum
+    
+    return rolled_tensor, num_tokens
+
+
 class MTPLossLoggingHelper:
     """Helper class for logging MTP losses."""
 
@@ -489,6 +564,7 @@ class MultiTokenPredictionLayer(MegatronModule):
         inference_params: InferenceParams = None,
         packed_seq_params: PackedSeqParams = None,
         sequence_len_offset: Tensor = None,
+        extra_block_kwargs: dict = None,
     ):
         """
         Perform the forward pass through the MTP layer.
@@ -518,9 +594,9 @@ class MultiTokenPredictionLayer(MegatronModule):
             [s, b, h], and optionally the updated context tensor if cross-attention is used.
         """
         assert context is None, f"multi token prediction + cross attention is not yet supported."
-        assert (
-            packed_seq_params is None
-        ), f"multi token prediction + sequence packing is not yet supported."
+        # assert (
+        #     packed_seq_params is None
+        # ), f"multi token prediction + sequence packing is not yet supported."
 
         hidden_states = make_viewless_tensor(inp=hidden_states, requires_grad=True, keep_graph=True)
 
@@ -758,17 +834,25 @@ class MultiTokenPredictionBlock(MegatronModule):
             # if loss_mask is not provided, use all ones as loss_mask
             loss_mask = torch.ones_like(labels)
 
-        hidden_states_main_model = hidden_states
+        # Preserve the main-model hidden states for RL loss (no detach), but optionally
+        # use a detached copy for the MTP branch to avoid backprop into the base model.
+        detach_for_mtp = bool(
+            extra_block_kwargs and extra_block_kwargs.get('mtp_detach_hidden_states', False)
+        )
+        hidden_states_main_model = hidden_states  # used for returning to main path
+        work_hidden_states = hidden_states.detach() if detach_for_mtp else hidden_states
         for layer_number in range(len(self.layers)):
             # Calc logits for the current Multi-Token Prediction (MTP) layers.
-            input_ids, _ = roll_tensor(input_ids, shifts=-1, dims=-1, cp_group=self.cp_group)
-            position_ids, _ = roll_tensor(position_ids, shifts=-1, dims=-1, cp_group=self.cp_group)
+            input_ids, _ = roll_tensor(input_ids, shifts=-1, dims=-1, cp_group=self.cp_group, packed_seq_params=packed_seq_params)
+            # There is no need to force position_ids to be not None for MTP.
+            if position_ids is not None:
+                position_ids, _ = roll_tensor(position_ids, shifts=-1, dims=-1, cp_group=self.cp_group, packed_seq_params=packed_seq_params)
             # embedding
             decoder_input = embedding(input_ids=input_ids, position_ids=position_ids)
             if self.config.recompute_granularity == 'full' and self.training:
-                hidden_states = self._checkpointed_forward(
+                work_hidden_states = self._checkpointed_forward(
                     layer_number=layer_number,
-                    hidden_states=hidden_states,
+                    hidden_states=work_hidden_states,
                     decoder_input=decoder_input,
                     attention_mask=attention_mask,
                     rotary_pos_emb=rotary_pos_emb,
@@ -781,8 +865,8 @@ class MultiTokenPredictionBlock(MegatronModule):
                 )
             else:
                 custom_forward = self._custom(layer_number)
-                hidden_states = custom_forward(
-                    hidden_states=hidden_states,
+                work_hidden_states = custom_forward(
+                    hidden_states=work_hidden_states,
                     decoder_input=decoder_input,
                     attention_mask=attention_mask,
                     rotary_pos_emb=rotary_pos_emb,
@@ -795,12 +879,12 @@ class MultiTokenPredictionBlock(MegatronModule):
                 )
             # output
             mtp_logits, _ = output_layer(
-                hidden_states, weight=output_weight, runtime_gather_output=runtime_gather_output
+                work_hidden_states, weight=output_weight, runtime_gather_output=runtime_gather_output
             )
             # Calc loss for the current Multi-Token Prediction (MTP) layers.
-            labels, _ = roll_tensor(labels, shifts=-1, dims=-1, cp_group=self.cp_group)
+            labels, _ = roll_tensor(labels, shifts=-1, dims=-1, cp_group=self.cp_group, packed_seq_params=packed_seq_params)
             loss_mask, num_tokens = roll_tensor(
-                loss_mask, shifts=-1, dims=-1, cp_group=self.cp_group
+                loss_mask, shifts=-1, dims=-1, cp_group=self.cp_group, packed_seq_params=packed_seq_params
             )
             mtp_loss = compute_language_model_loss(labels, mtp_logits)
             mtp_loss = loss_mask * mtp_loss
@@ -823,7 +907,39 @@ class MultiTokenPredictionBlock(MegatronModule):
 
         return hidden_states_main_model
 
-    def _checkpointed_forward(self, layer_number, *args, **kwargs):
+    def _checkpointed_forward(
+        self,
+        layer_number,
+        *,
+        hidden_states: Tensor,
+        decoder_input: Tensor,
+        attention_mask: Tensor,
+        rotary_pos_emb: Tensor = None,
+        rotary_pos_cos: Tensor = None,
+        rotary_pos_sin: Tensor = None,
+        inference_params: InferenceParams = None,
+        packed_seq_params: PackedSeqParams = None,
+        sequence_len_offset: Tensor = None,
+        extra_block_kwargs: dict = None,
+    ):
+        """Forward with activation checkpointing.
+
+        Only tensors are passed as checkpoint inputs; non-tensor arguments are
+        captured in the closure to satisfy CheckpointFunction's requirements.
+        """
+
+        # Original custom forward expecting keyword arguments
+        custom_forward = self._custom(layer_number)
+        wrapped_forward = partial(
+            custom_forward, 
+            rotary_pos_emb=rotary_pos_emb,
+            rotary_pos_cos=rotary_pos_cos,
+            rotary_pos_sin=rotary_pos_sin,
+            inference_params=inference_params,
+            packed_seq_params=packed_seq_params,
+            sequence_len_offset=sequence_len_offset,
+            extra_block_kwargs=extra_block_kwargs,
+        )
         def checkpoint_handler(forward_func):
             """Determines whether to use the `te_checkpoint` or `tensor_parallel.checkpoint`"""
             if self.config.fp8:
@@ -834,12 +950,17 @@ class MultiTokenPredictionBlock(MegatronModule):
                     self.config.distribute_saved_activations,
                     tensor_parallel.random.get_cuda_rng_tracker,
                     parallel_state.get_tensor_model_parallel_group(),
-                    *args,
-                    **kwargs,
+                    hidden_states,
+                    decoder_input,
+                    attention_mask,
                 )
             else:
                 return tensor_parallel.checkpoint(
-                    forward_func, self.config.distribute_saved_activations, *args, *kwargs.values()
+                    forward_func,
+                    self.config.distribute_saved_activations,
+                    hidden_states,
+                    decoder_input,
+                    attention_mask,
                 )
 
         if self.config.recompute_method == 'uniform':
@@ -849,13 +970,17 @@ class MultiTokenPredictionBlock(MegatronModule):
             assert (
                 self.config.recompute_num_layers == 1
             ), "recompute_num_layers must be 1 for MTP recompute"
-            hidden_states = checkpoint_handler(self._custom(layer_number))
+            hidden_states = checkpoint_handler(wrapped_forward)
         elif self.config.recompute_method == 'block':
             # TODO: implement block-based recompute for MTP
             warnings.warn(
-                "recompute_method == 'block' is not supported for MTP yet." " Skipping recompute."
+                "recompute_method == 'block' is not supported for MTP yet. Proceeding without checkpoint."
+            )
+            hidden_states = wrapped_forward(
+                hidden_states=hidden_states,
+                decoder_input=decoder_input,
+                attention_mask=attention_mask,
             )
-            hidden_states = self._custom(0, len(self.layers))(*args, **kwargs)
         else:
             raise ValueError("Invalid activation recompute method.")
 
@@ -885,7 +1010,7 @@ class MultiTokenPredictionBlock(MegatronModule):
                 rotary_pos_sin=rotary_pos_sin,
                 packed_seq_params=packed_seq_params,
                 sequence_len_offset=sequence_len_offset,
-                **(extra_block_kwargs or {}),
+                extra_block_kwargs=extra_block_kwargs,
             )
             return hidden_states
 
diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index 6f557e1f..b295fd35 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -173,6 +173,9 @@ class TransformerConfig(ModelParallelConfig):
     qk_layernorm: bool = False
     """Whether to apply `normalization` type of normalization to the query and key embeddings."""
 
+    post_self_attn_layernorm: bool = False
+    post_mlp_layernorm: bool = False
+
     test_mode: bool = False
     """Whether to run real-time tests."""
 
diff --git a/megatron/core/transformer/transformer_layer.py b/megatron/core/transformer/transformer_layer.py
index 84f22bde..b4807d26 100644
--- a/megatron/core/transformer/transformer_layer.py
+++ b/megatron/core/transformer/transformer_layer.py
@@ -224,6 +224,7 @@ class TransformerLayerSubmodules:
     input_layernorm: Union[ModuleSpec, type] = IdentityOp
     self_attention: Union[ModuleSpec, type] = IdentityOp
     self_attn_bda: Union[ModuleSpec, type] = IdentityFuncOp
+    post_self_attn_layernorm: Union[ModuleSpec, type] = IdentityOp
 
     pre_cross_attn_layernorm: Union[ModuleSpec, type] = IdentityOp
     cross_attention: Union[ModuleSpec, type] = IdentityOp
@@ -232,6 +233,7 @@ class TransformerLayerSubmodules:
     pre_mlp_layernorm: Union[ModuleSpec, type] = IdentityOp
     mlp: Union[ModuleSpec, type] = IdentityOp
     mlp_bda: Union[ModuleSpec, type] = IdentityFuncOp
+    post_mlp_layernorm: Union[ModuleSpec, type] = IdentityOp
 
     # Mapping for sharded tensor keys to be applied in `sharded_state_dict` method
     sharded_state_dict_keys_map: Dict[str, str] = field(default_factory=dict)
@@ -336,6 +338,14 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         # [Module 3: BiasDropoutFusion]
         self.self_attn_bda = build_module(submodules.self_attn_bda)
 
+        self.post_self_attn_layernorm = build_module(
+            submodules.post_self_attn_layernorm,
+            config=self.config,
+            hidden_size=self.config.hidden_size,
+            eps=self.config.layernorm_epsilon,
+        )
+
+
         # [Module 4: Post SelfAttention] Optional Layernorm after self-attn
         self.pre_cross_attn_layernorm = build_module(
             submodules.pre_cross_attn_layernorm,
@@ -399,6 +409,13 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         # [Module 9: BiasDropoutFusion]
         self.mlp_bda = build_module(submodules.mlp_bda)
 
+        self.post_mlp_layernorm = build_module(
+            submodules.post_mlp_layernorm,
+            config=self.config,
+            hidden_size=self.config.hidden_size,
+            eps=self.config.layernorm_epsilon
+        )
+
         self.recompute_input_layernorm = False
         self.recompute_pre_mlp_layernorm = False
         self.recompute_mlp = False
@@ -535,6 +552,11 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
                 attention_output_with_bias[0]
             )
 
+        attention_output, attention_output_bias = attention_output_with_bias
+        attention_output = self.post_self_attn_layernorm(attention_output)
+        attention_output_with_bias = (attention_output, attention_output_bias)
+
+
         # TODO: could we move `bias_dropout_add_exec_handler` itself
         # inside the module provided in the `bias_dropout_add_spec` module?
         nvtx_range_push(suffix="self_attn_bda")
@@ -635,6 +657,10 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         else:
             mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)
 
+        mlp_output, mlp_output_bias = mlp_output_with_bias
+        mlp_output = self.post_mlp_layernorm(mlp_output)
+        mlp_output_with_bias = (mlp_output, mlp_output_bias)
+
         if self.recompute_pre_mlp_layernorm:
             # discard the output of the pre-mlp layernorm and register the recompute
             # as a gradient hook of mlp_output_with_bias[0]
diff --git a/megatron/training/arguments.py b/megatron/training/arguments.py
index 24ba8926..4f039fd4 100644
--- a/megatron/training/arguments.py
+++ b/megatron/training/arguments.py
@@ -1191,6 +1191,9 @@ def core_transformer_config_from_args(args, config_class=None):
     if args.is_hybrid_model:
         kw_args['is_hybrid_model'] = args.is_hybrid_model
 
+    kw_args['post_self_attn_layernorm'] = args.post_self_attn_layernorm
+    kw_args['post_mlp_layernorm'] = args.post_mlp_layernorm
+
     # handle quantization config
     # NOTE: Kitchen arguments are only added to the namespace when
     # Kitchen library is available.
@@ -1481,6 +1484,10 @@ def _add_network_size_args(parser):
                        action='store_true',
                        help='If set, use original BERT residula connection '
                        'ordering.')
+    group.add_argument('--post-self-attn-layernorm', action='store_true',
+                       help='If set, use post self attention layernorm.')
+    group.add_argument('--post-mlp-layernorm', action='store_true',
+                       help='If set, use post MLP layernorm.')
     group.add_argument('--openai-gelu', action='store_true',
                        help='Use OpenAIs GeLU implementation. This option'
                        'should not be used unless for backward compatibility'
